{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from ast import literal_eval\nimport re\nfrom itertools import chain\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModel, AutoTokenizer\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:07.029076Z","iopub.execute_input":"2022-04-25T06:34:07.029397Z","iopub.status.idle":"2022-04-25T06:34:14.556565Z","shell.execute_reply.started":"2022-04-25T06:34:07.029327Z","shell.execute_reply":"2022-04-25T06:34:14.555858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#display options\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:14.558079Z","iopub.execute_input":"2022-04-25T06:34:14.558321Z","iopub.status.idle":"2022-04-25T06:34:14.565063Z","shell.execute_reply.started":"2022-04-25T06:34:14.558278Z","shell.execute_reply":"2022-04-25T06:34:14.564055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NBME - Score Clinical Patient Notes : analysis","metadata":{}},{"cell_type":"markdown","source":"# NBME - Score Clinical Patient Notes : modeling\n- **Framework:** Pytorch\n- **Model Architecture:**\n    - BERT\n    - Linear(768, 512)\n    - Linear(512, 1)\n- **LR:** 1e-5\n- **Batch Size:** 8\n- **Epoch:** 6\n- **Dropout:** 0.2\n- **Criterion:** BCEWithLogitsLoss\n- **Optimizer:** AdamW\n\n# Tokenizer params\n- **Max Lenght:** 416\n- **Padding:** max_lenght\n- **Truncation（截断）:** only_scond\n","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{}},{"cell_type":"code","source":"hyperparameters = {\n    \"max_length\": 416,\n    \"padding\": \"max_length\",\n    \"return_offsets_mapping\": True,\n    \"truncation\": \"only_second\",\n    \"model_name\": (\"../input/layoutlm/BiomedNLP-PubMedBERT\"\n                   \"-base-uncased-abstract-fulltext\"),\n    \"debug\":False,\n    \"dropout\": 0.2,\n    \"encoder_lr\": 1e-5,\n    \"decoder_lr\":1e-5,\n    \"weight_decay\":0.01,\n    'betas':(0.9, 0.999),\n\n#     \"test_size\": 0.8,\n    \"seed\": 1268,\n    \"batch_size\": 8,\n    \n    'apex': True,\n#     'gradient_accumulation_steps':1,\n#     'batch_scheduler':True,\n#     'scheduler':'cosine',\n#     'num_warmup_steps':0,\n#     'num_cycles':0.5,\n    'epochs':3,\n    'eps':1e-6,\n    \n    \"n_fold\":5,\n    \"trn_fold\":[1,2,3,4,5],\n    \"hidden_size\":768\n\n}\n\nif hyperparameters['debug']:\n    hyperparameters['epochs'] = 2\n    hyperparameters['trn_fold'] = [1,2]\n\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:14.566211Z","iopub.execute_input":"2022-04-25T06:34:14.56644Z","iopub.status.idle":"2022-04-25T06:34:14.579451Z","shell.execute_reply.started":"2022-04-25T06:34:14.566408Z","shell.execute_reply":"2022-04-25T06:34:14.57869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions\n### 1. Datasets Helper Function\nneed to merge `features.csv`, `patient_notes.csv` with `train.csv`","metadata":{}},{"cell_type":"code","source":"BASE_URL = \"../input/nbme-score-clinical-patient-notes\"\n\n\ndef process_feature_text(text):\n    return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n\n\ndef clean_spaces(text):\n    text = re.sub('\\n', ' ', text)\n    text = re.sub('\\t', ' ', text)\n    text = re.sub('\\r', ' ', text)\n#     txt = re.sub(r'\\s+', ' ', txt)\n    return text\n\ndef prepare_datasets():\n    features = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    df = pd.read_csv(f\"{BASE_URL}/train.csv\")\n\n    df[\"annotation_list\"] = [literal_eval(x) for x in df[\"annotation\"]]\n    df[\"location_list\"] = [literal_eval(x) for x in df[\"location\"]]\n\n    df.loc[338, 'annotation_list'] = literal_eval('[[\"father heart attack\"]]')\n    df.loc[338, 'location_list'] = literal_eval('[[\"764 783\"]]')\n    df.loc[621, 'annotation_list'] = literal_eval('[[\"for the last 2-3 months\"]]')\n    df.loc[621, 'location_list'] = literal_eval('[[\"77 100\"]]')\n    df.loc[655, 'annotation_list'] = literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n    df.loc[655, 'location_list'] = literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n    df.loc[1262, 'annotation_list'] = literal_eval('[[\"mother thyroid problem\"]]')\n    df.loc[1262, 'location_list'] = literal_eval('[[\"551 557;565 580\"]]')\n    df.loc[1265, 'annotation_list'] = literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n    df.loc[1265, 'location_list'] = literal_eval('[[\"131 135;181 212\"]]')\n    df.loc[1396, 'annotation_list'] = literal_eval('[[\"stool , with no blood\"]]')\n    df.loc[1396, 'location_list'] = literal_eval('[[\"259 280\"]]')\n    df.loc[1591, 'annotation_list'] = literal_eval('[[\"diarrhoe non blooody\"]]')\n    df.loc[1591, 'location_list'] = literal_eval('[[\"176 184;201 212\"]]')\n    df.loc[1615, 'annotation_list'] = literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n    df.loc[1615, 'location_list'] = literal_eval('[[\"249 257;271 288\"]]')\n    df.loc[1664, 'annotation_list'] = literal_eval('[[\"no vaginal discharge\"]]')\n    df.loc[1664, 'location_list'] = literal_eval('[[\"822 824;907 924\"]]')\n    df.loc[1714, 'annotation_list'] = literal_eval('[[\"started about 8-10 hours ago\"]]')\n    df.loc[1714, 'location_list'] = literal_eval('[[\"101 129\"]]')\n    df.loc[1929, 'annotation_list'] = literal_eval('[[\"no blood in the stool\"]]')\n    df.loc[1929, 'location_list'] = literal_eval('[[\"531 539;549 561\"]]')\n    df.loc[2134, 'annotation_list'] = literal_eval('[[\"last sexually active 9 months ago\"]]')\n    df.loc[2134, 'location_list'] = literal_eval('[[\"540 560;581 593\"]]')\n    df.loc[2191, 'annotation_list'] = literal_eval('[[\"right lower quadrant pain\"]]')\n    df.loc[2191, 'location_list'] = literal_eval('[[\"32 57\"]]')\n    df.loc[2553, 'annotation_list'] = literal_eval('[[\"diarrhoea no blood\"]]')\n    df.loc[2553, 'location_list'] = literal_eval('[[\"308 317;376 384\"]]')\n    df.loc[3124, 'annotation_list'] = literal_eval('[[\"sweating\"]]')\n    df.loc[3124, 'location_list'] = literal_eval('[[\"549 557\"]]')\n    df.loc[3858, 'annotation_list'] = literal_eval(\n        '[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n    df.loc[3858, 'location_list'] = literal_eval(\n        '[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n    df.loc[4373, 'annotation_list'] = literal_eval('[[\"for 2 months\"]]')\n    df.loc[4373, 'location_list'] = literal_eval('[[\"33 45\"]]')\n    df.loc[4763, 'annotation_list'] = literal_eval('[[\"35 year old\"]]')\n    df.loc[4763, 'location_list'] = literal_eval('[[\"5 16\"]]')\n    df.loc[4782, 'annotation_list'] = literal_eval('[[\"darker brown stools\"]]')\n    df.loc[4782, 'location_list'] = literal_eval('[[\"175 194\"]]')\n    df.loc[4908, 'annotation_list'] = literal_eval('[[\"uncle with peptic ulcer\"]]')\n    df.loc[4908, 'location_list'] = literal_eval('[[\"700 723\"]]')\n    df.loc[6016, 'annotation_list'] = literal_eval('[[\"difficulty falling asleep\"]]')\n    df.loc[6016, 'location_list'] = literal_eval('[[\"225 250\"]]')\n    df.loc[6192, 'annotation_list'] = literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n    df.loc[6192, 'location_list'] = literal_eval('[[\"197 218;236 260\"]]')\n    df.loc[6380, 'annotation_list'] = literal_eval(\n        '[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n    df.loc[6380, 'location_list'] = literal_eval(\n        '[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n    df.loc[6562, 'annotation_list'] = literal_eval(\n        '[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n    df.loc[6562, 'location_list'] = literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n    df.loc[6862, 'annotation_list'] = literal_eval('[[\"stressor taking care of many sick family members\"]]')\n    df.loc[6862, 'location_list'] = literal_eval('[[\"288 296;324 363\"]]')\n    df.loc[7022, 'annotation_list'] = literal_eval(\n        '[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n    df.loc[7022, 'location_list'] = literal_eval('[[\"108 182\"]]')\n    df.loc[7422, 'annotation_list'] = literal_eval('[[\"first started 5 yrs\"]]')\n    df.loc[7422, 'location_list'] = literal_eval('[[\"102 121\"]]')\n    df.loc[8876, 'annotation_list'] = literal_eval('[[\"No shortness of breath\"]]')\n    df.loc[8876, 'location_list'] = literal_eval('[[\"481 483;533 552\"]]')\n    df.loc[9027, 'annotation_list'] = literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n    df.loc[9027, 'location_list'] = literal_eval('[[\"92 102\"], [\"123 164\"]]')\n    df.loc[9938, 'annotation_list'] = literal_eval(\n        '[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n    df.loc[9938, 'location_list'] = literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n    df.loc[9973, 'annotation_list'] = literal_eval('[[\"gaining 10-15 lbs\"]]')\n    df.loc[9973, 'location_list'] = literal_eval('[[\"344 361\"]]')\n    df.loc[10513, 'annotation_list'] = literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n    df.loc[10513, 'location_list'] = literal_eval('[[\"600 611\"], [\"607 623\"]]')\n    df.loc[11551, 'annotation_list'] = literal_eval('[[\"seeing her son knows are not real\"]]')\n    df.loc[11551, 'location_list'] = literal_eval('[[\"386 400;443 461\"]]')\n    df.loc[11677, 'annotation_list'] = literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n    df.loc[11677, 'location_list'] = literal_eval('[[\"160 201\"]]')\n    df.loc[12124, 'annotation_list'] = literal_eval('[[\"tried Ambien but it didnt work\"]]')\n    df.loc[12124, 'location_list'] = literal_eval('[[\"325 337;349 366\"]]')\n    df.loc[12279, 'annotation_list'] = literal_eval(\n        '[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n    df.loc[12279, 'location_list'] = literal_eval('[[\"405 459;488 524\"]]')\n    df.loc[12289, 'annotation_list'] = literal_eval(\n        '[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n    df.loc[12289, 'location_list'] = literal_eval('[[\"353 400;488 524\"]]')\n    df.loc[13238, 'annotation_list'] = literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n    df.loc[13238, 'location_list'] = literal_eval('[[\"293 307\"], [\"321 331\"]]')\n    df.loc[13297, 'annotation_list'] = literal_eval(\n        '[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n    df.loc[13297, 'location_list'] = literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n    df.loc[13299, 'annotation_list'] = literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n    df.loc[13299, 'location_list'] = literal_eval('[[\"79 88\"], [\"409 418\"]]')\n    df.loc[13845, 'annotation_list'] = literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n    df.loc[13845, 'location_list'] = literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n    df.loc[14083, 'annotation_list'] = literal_eval('[[\"headache generalized in her head\"]]')\n    df.loc[14083, 'location_list'] = literal_eval('[[\"56 64;156 179\"]]')\n\n    merged = df.merge(notes, how=\"left\")\n    merged = merged.merge(features, how=\"left\")\n\n    merged[\"feature_text\"] = [process_feature_text(x) for x in\n                              merged[\"feature_text\"]]\n#     train['feature_text'] = train['feature_text'].apply(clean_spaces)\n#     train['clean_text'] = train['pn_history'].apply(clean_spaces)\n#     train['pn_history'] = train['pn_history'].apply(lambda x: x.strip())\n    #增加数据预处理\n    merged['feature_text'] = merged['feature_text'].apply(clean_spaces)\n    merged['pn_history'] = merged['pn_history'].apply(clean_spaces)\n    merged['pn_history'] = merged['pn_history'].apply(lambda x: x.strip())\n    \n    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n    #debug模式\n    if hyperparameters['debug']:\n        merged = merged.sample(n=1000,random_state=0).reset_index(drop=True)\n    return merged\n\n\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:14.581814Z","iopub.execute_input":"2022-04-25T06:34:14.582154Z","iopub.status.idle":"2022-04-25T06:34:14.62043Z","shell.execute_reply.started":"2022-04-25T06:34:14.582116Z","shell.execute_reply":"2022-04-25T06:34:14.619582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Tokenizer Helper Function","metadata":{}},{"cell_type":"code","source":"def loc_list_to_ints(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\n\ndef tokenize_and_add_labels(tokenizer, data, config):\n    out = tokenizer(\n        data[\"feature_text\"],\n        data[\"pn_history\"],\n        truncation=config['truncation'],\n        max_length=config['max_length'],\n        padding=config['padding'],\n        return_offsets_mapping=config['return_offsets_mapping']\n    )\n    labels = [0.0] * len(out[\"input_ids\"])\n    out[\"location_int\"] = loc_list_to_ints(data[\"location_list\"])\n    out[\"sequence_ids\"] = out.sequence_ids()\n\n    for idx, (seq_id, offsets) in enumerate(zip(out[\"sequence_ids\"],\n                                                out[\"offset_mapping\"])):\n        if not seq_id or seq_id == 0:\n            labels[idx] = -1\n            continue\n\n        token_start, token_end = offsets\n        for feature_start, feature_end in out[\"location_int\"]:\n            if token_start >= feature_start and token_end <= feature_end:\n                labels[idx] = 1.0\n                break\n\n    out[\"labels\"] = labels\n\n    return out\n\n\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:14.621319Z","iopub.execute_input":"2022-04-25T06:34:14.621505Z","iopub.status.idle":"2022-04-25T06:34:14.632773Z","shell.execute_reply.started":"2022-04-25T06:34:14.621474Z","shell.execute_reply":"2022-04-25T06:34:14.63199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Prediction and Score Helper Function","metadata":{}},{"cell_type":"code","source":"def get_location_predictions(preds, offset_mapping, sequence_ids, test=False):\n    all_predictions = []\n    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n        if not test:\n            pred = 1 / (1 + np.exp(-pred))\n        else:\n            pass\n        start_idx = None\n        end_idx = None\n        current_preds = []\n        for pred, offset, seq_id in zip(pred, offsets, seq_ids):\n            if seq_id is None or seq_id == 0:\n                continue\n\n            if pred > 0.5:\n                if start_idx is None:\n                    start_idx = offset[0]\n                end_idx = offset[1]\n            elif start_idx is not None:\n                #增加if语句筛选0 0的状况\n                if test:\n                    if start_idx==0 and end_idx==0:\n                        start_idx = None\n                        continue\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n            \n    return all_predictions\n\n\ndef calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping,\n                                               sequence_ids, labels):\n\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros(num_chars)\n\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n\n        char_preds = np.zeros(num_chars)\n\n        for start_idx, end_idx in preds:\n            char_preds[start_idx:end_idx] = 1\n            \n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n\n    results = precision_recall_fscore_support(all_labels,\n                                              all_preds,\n                                              average=\"binary\",\n                                              labels=np.unique(all_preds))\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n\n    return {\n        \"Accuracy\": accuracy,\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }\n\n\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:14.634046Z","iopub.execute_input":"2022-04-25T06:34:14.63444Z","iopub.status.idle":"2022-04-25T06:34:14.649772Z","shell.execute_reply.started":"2022-04-25T06:34:14.634406Z","shell.execute_reply":"2022-04-25T06:34:14.648993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        tokens = tokenize_and_add_labels(self.tokenizer, data, self.config)\n\n        input_ids = np.array(tokens[\"input_ids\"])\n        attention_mask = np.array(tokens[\"attention_mask\"])\n        token_type_ids = np.array(tokens[\"token_type_ids\"])\n\n        labels = np.array(tokens[\"labels\"])\n        offset_mapping = np.array(tokens['offset_mapping'])\n        sequence_ids = np.array(tokens['sequence_ids']).astype(\"float16\")\n        \n        return input_ids, attention_mask, token_type_ids, labels, offset_mapping, sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:14.652634Z","iopub.execute_input":"2022-04-25T06:34:14.653341Z","iopub.status.idle":"2022-04-25T06:34:14.661588Z","shell.execute_reply.started":"2022-04-25T06:34:14.653282Z","shell.execute_reply":"2022-04-25T06:34:14.660946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- Lets use **Pub med BERT** base Architecture downloaded at https://www.kaggle.com/jpmiller/layoutlm\n- Also Used 2 FC layers\n","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nclass CustomModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.model = AutoModel.from_pretrained(config['model_name'])\n        self.dropout = nn.Dropout(p=config['dropout'])\n        self.config = config\n#         self.fc1 = nn.Linear(768, 512)\n#         self.fc2 = nn.Linear(512,512)\n#         self.fc3 = nn.Linear(512, 1)\n        self.fc = nn.Linear(hyperparameters['hidden_size'], 1)\n        self.gru = nn.GRU(768,384,num_layers=2,\\\n                bidirectional=True,batch_first=True)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.model(input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids)\n#         logits = F.relu(self.fc1(outputs[0]))\n#         logits = F.relu(self.fc2(self.dropout(logits)))\n#         logits = self.fc3(self.dropout(logits)).squeeze(-1)\n        enc,_ = self.gru(outputs[0])\n        logits = self.fc(self.dropout(enc)).squeeze(-1)\n        return logits\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:14.662981Z","iopub.execute_input":"2022-04-25T06:34:14.663254Z","iopub.status.idle":"2022-04-25T06:34:14.673458Z","shell.execute_reply.started":"2022-04-25T06:34:14.663219Z","shell.execute_reply":"2022-04-25T06:34:14.672876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scheduler\n# def get_scheduler(hyperparameters, optimizer, num_train_steps):\n#     if hyperparameters['scheduler'] == 'linear':\n#         scheduler = get_linear_schedule_with_warmup(\n#             optimizer, num_warmup_steps=hyperparameters['num_warmup_steps'], num_training_steps=num_train_steps\n#         )\n#     elif hyperparameters['scheduler'] == 'cosine':\n#         scheduler = get_cosine_schedule_with_warmup(\n#             optimizer, num_warmup_steps=hyperparameters['num_warmup_steps'], num_training_steps=num_train_steps,\n#             num_cycles=hyperparameters['num_cycles']\n#         )\n#     return scheduler","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:14.674656Z","iopub.execute_input":"2022-04-25T06:34:14.677007Z","iopub.status.idle":"2022-04-25T06:34:14.681714Z","shell.execute_reply.started":"2022-04-25T06:34:14.676966Z","shell.execute_reply":"2022-04-25T06:34:14.681019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Datasets and Train\n- Train and Test split: 20% for 5-fold cross validation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfold = KFold(n_splits=hyperparameters['n_fold'],shuffle=True,random_state=1268)\ntrain_df = prepare_datasets()\n\n# X_train, X_test = train_test_split(train_df,\n#                                    test_size=hyperparameters['test_size'],\n#                                    random_state=hyperparameters['seed'])\n\n\n# print(\"Train size\", len(X_train))\n# print(\"Test Size\", len(X_test))\n\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:14.684521Z","iopub.execute_input":"2022-04-25T06:34:14.685083Z","iopub.status.idle":"2022-04-25T06:34:15.978929Z","shell.execute_reply.started":"2022-04-25T06:34:14.685047Z","shell.execute_reply":"2022-04-25T06:34:15.978064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(hyperparameters['model_name'])\n\n# training_data = CustomDataset(X_train,\n#                               tokenizer,\n#                               hyperparameters)\n\n# train_dataloader = DataLoader(training_data,\n#                               batch_size=hyperparameters['batch_size'],\n#                               shuffle=True,pin_memory=True)\n\n# test_data = CustomDataset(X_test, tokenizer, hyperparameters)\n\n# test_dataloader = DataLoader(test_data,\n#                              batch_size=hyperparameters['batch_size'],\n#                              shuffle=False,pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:15.980209Z","iopub.execute_input":"2022-04-25T06:34:15.982275Z","iopub.status.idle":"2022-04-25T06:34:16.053133Z","shell.execute_reply.started":"2022-04-25T06:34:15.982242Z","shell.execute_reply":"2022-04-25T06:34:16.05242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets train the model\nwith BCEWithLogitsLoss and AdamW as optimizer\n\n**Notes:** on BCEWithLogitsLoss, the default value for reduction is `mean` (the sum of the output will be divided by the number of elements in the output). If we use this default value, it will produce negative loss. Because we have some negative labels. To fix this negative loss issue, we can use `none` as parameter. To calculate the mean, first, we have to filter out the negative values. [DOC](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)","metadata":{}},{"cell_type":"code","source":"#优化器函数\ndef get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay):\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n         'lr': encoder_lr, 'weight_decay': weight_decay},\n        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n         'lr': encoder_lr, 'weight_decay': 0.0},\n        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n         'lr': decoder_lr, 'weight_decay': 0.0}\n    ]\n    return optimizer_parameters","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:16.054312Z","iopub.execute_input":"2022-04-25T06:34:16.05464Z","iopub.status.idle":"2022-04-25T06:34:16.061929Z","shell.execute_reply.started":"2022-04-25T06:34:16.0546Z","shell.execute_reply":"2022-04-25T06:34:16.061144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, criterion):\n    #,scheduler\n    model.train()\n    train_loss = []\n    #scaler = torch.cuda.amp.GradScaler(enabled=hyperparameters['apex'])\n    global_step = 0\n    for step,batch in enumerate(tqdm(dataloader)):\n        optimizer.zero_grad()\n        #(8,416),context和question的词表索引+padding（0）\n        input_ids = batch[0].to(DEVICE)\n        #(8,416) valid ,0或1,1表示需要attention计算（包括context和queation）\n        attention_mask = batch[1].to(DEVICE)\n        #(8,416)，0或1，区分(context,queation)和其他序列(包括padding,[cls][seq])\n        token_type_ids = batch[2].to(DEVICE)\n        #（8，416），context和question为0，其中答案为1，其他为0（包括padding,[cls][seq]）\n        labels = batch[3].to(DEVICE)\n\n        #(8,416)\n        logits = model(input_ids,\n                       attention_mask,\n                       token_type_ids)\n        #(8,416),因为实例化时添加reduction=None\n        loss = criterion(logits, labels)\n        #计算平均loss忽略labels=-1的位置\n        #标量\n        loss = torch.masked_select(loss, labels > -1.0).mean()\n        train_loss.append(loss.item() * input_ids.size(0))\n        loss.backward()\n        #scaler.scale(loss).backward()\n        # clip the the gradients to 1.0.\n        # It helps in preventing the exploding gradient problem\n        # it's also improve f1 accuracy slightly\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n#         if (step+1) % hyperparameters['gradient_accumulation_steps'] ==0:\n#             scaler.step(optimizer)\n#             scaler.update()\n#             optimizer.zero_grad()\n#             global_step+=1\n#             if hyperparameters['batch_scheduler']:\n#                 scheduler.step()\n\n    return sum(train_loss) / len(train_loss)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:16.063292Z","iopub.execute_input":"2022-04-25T06:34:16.063726Z","iopub.status.idle":"2022-04-25T06:34:16.076641Z","shell.execute_reply.started":"2022-04-25T06:34:16.063631Z","shell.execute_reply":"2022-04-25T06:34:16.075972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, dataloader, criterion):\n        model.eval()\n        valid_loss = []\n        preds = []\n        offsets = []\n        seq_ids = []\n        valid_labels = []\n\n        for batch in tqdm(dataloader):\n            input_ids = batch[0].to(DEVICE)\n            attention_mask = batch[1].to(DEVICE)\n            token_type_ids = batch[2].to(DEVICE)\n            labels = batch[3].to(DEVICE)\n            offset_mapping = batch[4]\n            sequence_ids = batch[5]\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = criterion(logits, labels)\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            valid_loss.append(loss.item() * input_ids.size(0))\n\n            preds.append(logits.detach().cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n            valid_labels.append(labels.detach().cpu().numpy())\n\n        preds = np.concatenate(preds, axis=0)\n        offsets = np.concatenate(offsets, axis=0)\n        seq_ids = np.concatenate(seq_ids, axis=0)\n        valid_labels = np.concatenate(valid_labels, axis=0)\n        location_preds = get_location_predictions(preds,\n                                                  offsets,\n                                                  seq_ids,\n                                                  test=False)\n        score = calculate_char_cv(location_preds,\n                                  offsets,\n                                  seq_ids,\n                                  valid_labels)\n\n        return sum(valid_loss)/len(valid_loss), score\n\n\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:16.078235Z","iopub.execute_input":"2022-04-25T06:34:16.078492Z","iopub.status.idle":"2022-04-25T06:34:16.091385Z","shell.execute_reply.started":"2022-04-25T06:34:16.078459Z","shell.execute_reply":"2022-04-25T06:34:16.090709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nsince = time.time()\nfrom torch.utils.data import random_split,SubsetRandomSampler\nimport gc\nfold_num = 0\ndf_data = CustomDataset(train_df,tokenizer,hyperparameters)\nbest_loss = np.inf\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nfor train_index,test_index in fold.split(train_df):\n    if (fold_num+1) in hyperparameters['trn_fold']:\n        train_sampler = SubsetRandomSampler(train_index)\n        test_sampler = SubsetRandomSampler(test_index)\n        print(f'第{fold_num+1}折模型')\n        print(f'训练集大小：{len(train_index)}\\n',f'测试集大小:{len(test_index)}')\n        #dataloader\n        train_dataloader = DataLoader(df_data,\n                                     batch_size=hyperparameters['batch_size'],\n                                     shuffle=False,pin_memory=True,sampler=train_sampler)\n        test_dataloader = DataLoader(df_data,\n                                     batch_size=hyperparameters['batch_size'],\n                                     shuffle=False,pin_memory=True,sampler=test_sampler)\n        #初始化各种参数\n        train_loss_data, valid_loss_data = [], []\n        score_data_list = []\n        valid_loss_min = np.Inf\n        #初始化模型、loss、优化器\n        model = CustomModel(hyperparameters).to(DEVICE)\n        criterion = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n        optimizer_parameters = get_optimizer_params(model,\n                                            encoder_lr=hyperparameters['encoder_lr'],\n                                            decoder_lr=hyperparameters['decoder_lr'],\n                                            weight_decay=hyperparameters['weight_decay'])\n        optimizer = optim.AdamW(optimizer_parameters, lr=hyperparameters['encoder_lr'],eps=hyperparameters['eps'],betas=hyperparameters['betas'])\n        #训练\n        for i in range(hyperparameters['epochs']):\n            print(\"Epoch: {}/{}\".format(i + 1, hyperparameters['epochs']))\n            # first train model\n            train_loss = train_model(model, train_dataloader, optimizer, criterion)\n            #,scheduler\n            train_loss_data.append(train_loss)\n            print(f\"Train loss: {train_loss}\")\n            #evaluate model\n            valid_loss, score = eval_model(model, test_dataloader, criterion)\n            valid_loss_data.append(valid_loss)\n            score_data_list.append(score)\n            print(f\"Valid loss: {valid_loss}\")\n            print(f\"Valid score: {score}\")\n        #保存最优模型\n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                torch.save(model.state_dict(), f\"nbme_pubmed_bert_fold{fold_num+1}.pth\")\n        best_loss = np.inf\n        fold_num+=1\n        torch.cuda.empty_cache()\n        gc.collect()\n    else:\n        fold_num+=1\n        train_loss_data, valid_loss_data = [], []\n        score_data_list = []\n        valid_loss_min = np.Inf\n        best_loss = np.inf\n\ntime_elapsed = time.time() - since\nprint('Training completed in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:34:16.092583Z","iopub.execute_input":"2022-04-25T06:34:16.092889Z","iopub.status.idle":"2022-04-25T06:37:29.417369Z","shell.execute_reply.started":"2022-04-25T06:34:16.092852Z","shell.execute_reply":"2022-04-25T06:37:29.416622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from matplotlib import pyplot as plt\n\n# plt.plot(train_loss_data, label=\"Training loss\")\n# plt.plot(valid_loss_data, label=\"validation loss\")\n# plt.legend(frameon=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:37:29.418469Z","iopub.execute_input":"2022-04-25T06:37:29.420036Z","iopub.status.idle":"2022-04-25T06:37:29.424256Z","shell.execute_reply.started":"2022-04-25T06:37:29.419996Z","shell.execute_reply":"2022-04-25T06:37:29.423208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nscore_df = pd.DataFrame.from_dict(score_data_list)\nscore_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:37:29.425715Z","iopub.execute_input":"2022-04-25T06:37:29.42598Z","iopub.status.idle":"2022-04-25T06:37:29.44233Z","shell.execute_reply.started":"2022-04-25T06:37:29.425945Z","shell.execute_reply":"2022-04-25T06:37:29.441599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare For Testing","metadata":{}},{"cell_type":"markdown","source":"Load best model","metadata":{}},{"cell_type":"code","source":"#model.load_state_dict(torch.load(\"nbme_pubmed_bert.pth\", map_location = DEVICE))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:37:29.443337Z","iopub.execute_input":"2022-04-25T06:37:29.444125Z","iopub.status.idle":"2022-04-25T06:37:29.447987Z","shell.execute_reply.started":"2022-04-25T06:37:29.444083Z","shell.execute_reply":"2022-04-25T06:37:29.447088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_df():\n    feats = pd.read_csv(f\"{BASE_URL}/features.csv\")\n    notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv\")\n    test = pd.read_csv(f\"{BASE_URL}/test.csv\")\n\n    merged = test.merge(notes, how=\"left\")\n    merged = merged.merge(feats, how=\"left\")\n\n    def process_feature_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n    \n    merged[\"feature_text\"] = [process_feature_text(x) \n                              for x in merged[\"feature_text\"]]\n    \n    return merged\n\n\nclass SubmissionDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data.loc[idx]\n        tokenized = self.tokenizer(\n            example[\"feature_text\"],\n            example[\"pn_history\"],\n            truncation = self.config['truncation'],\n            max_length = self.config['max_length'],\n            padding = self.config['padding'],\n            return_offsets_mapping = self.config['return_offsets_mapping']\n        )\n        tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n\n        input_ids = np.array(tokenized[\"input_ids\"])\n        attention_mask = np.array(tokenized[\"attention_mask\"])\n        token_type_ids = np.array(tokenized[\"token_type_ids\"])\n        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n        sequence_ids = np.array(tokenized[\"sequence_ids\"])\\\n                        .astype(\"float16\")\n\n        return input_ids, attention_mask, token_type_ids, offset_mapping, sequence_ids\n\n\ntest_df = create_test_df()\n\nsubmission_data = SubmissionDataset(test_df, tokenizer, hyperparameters)\nsubmission_dataloader = DataLoader(submission_data,\n                                   batch_size=hyperparameters['batch_size'],\n                                   shuffle=False)\n\n#pep8(_ih)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T06:37:29.449223Z","iopub.execute_input":"2022-04-25T06:37:29.449659Z","iopub.status.idle":"2022-04-25T06:37:29.756683Z","shell.execute_reply.started":"2022-04-25T06:37:29.449624Z","shell.execute_reply":"2022-04-25T06:37:29.755943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#准备分折预测\npredictions = []\nfor fold in hyperparameters['trn_fold']:\n    model = CustomModel(hyperparameters).to(DEVICE)\n    model.load_state_dict(torch.load(f\"nbme_pubmed_bert_fold{fold}.pth\"))\n    model.eval()\n    #初始化指标容器\n    preds = []\n    offsets = []\n    seq_ids = []\n#     logits_container = []\n    for batch in tqdm(submission_dataloader):\n        input_ids = batch[0].to(DEVICE)\n        attention_mask = batch[1].to(DEVICE)\n        token_type_ids = batch[2].to(DEVICE)\n        offset_mapping = batch[3]\n        sequence_ids = batch[4]\n        logits = model(input_ids, attention_mask, token_type_ids)\n#         logits_container.append(logits)\n        preds.append(logits.sigmoid().detach().cpu().numpy())\n        offsets.append(offset_mapping.numpy())\n        seq_ids.append(sequence_ids.numpy())\n    \n    preds = np.concatenate(preds, axis=0)\n    predictions.append(preds)\n    offsets = np.concatenate(offsets, axis=0)\n    seq_ids = np.concatenate(seq_ids, axis=0)\n    \n    del model,preds; gc.collect()\n    torch.cuda.empty_cache()\n#五折总输出的平均\npredictions = np.mean(predictions,axis=0)\n\n#输出\nlocation_preds = get_location_predictions(predictions, offsets, seq_ids, test=True)\ntest_df[\"location\"] = location_preds\ntest_df[[\"id\", \"location\"]].to_csv(\"submission.csv\", index = False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-25T06:42:52.756848Z","iopub.execute_input":"2022-04-25T06:42:52.757349Z","iopub.status.idle":"2022-04-25T06:42:56.403055Z","shell.execute_reply.started":"2022-04-25T06:42:52.757311Z","shell.execute_reply":"2022-04-25T06:42:56.40238Z"},"trusted":true},"execution_count":null,"outputs":[]}]}